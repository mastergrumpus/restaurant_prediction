{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b0d8ad7-80f4-4263-9c5e-3e591871c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from preprocessor_class import Preprocessor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, BaggingClassifier, VotingClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score, log_loss, accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e87473-7a1e-4300-b48f-cfbc65525488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Menu_Item_ID</th>\n",
       "      <th>Year</th>\n",
       "      <th>Restaurant_Item_Name</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>Restaurant_ID</th>\n",
       "      <th>Item_Name</th>\n",
       "      <th>Item_Description</th>\n",
       "      <th>Food_Category</th>\n",
       "      <th>Serving_Size</th>\n",
       "      <th>Serving_Size_text</th>\n",
       "      <th>...</th>\n",
       "      <th>Sodium_text</th>\n",
       "      <th>Potassium_text</th>\n",
       "      <th>Carbohydrates_text</th>\n",
       "      <th>Protein_text</th>\n",
       "      <th>Sugar_text</th>\n",
       "      <th>Dietary_Fiber_text</th>\n",
       "      <th>Kids_Meal</th>\n",
       "      <th>Limited_Time_Offer</th>\n",
       "      <th>Regional</th>\n",
       "      <th>Shareable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35005</td>\n",
       "      <td>2017</td>\n",
       "      <td>7 Eleven Mocha Iced Coffee</td>\n",
       "      <td>7 Eleven</td>\n",
       "      <td>1</td>\n",
       "      <td>Mocha Iced Coffee</td>\n",
       "      <td>Mocha Iced Coffee, Chillers Iced Coffee, Drinks</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35008</td>\n",
       "      <td>2017</td>\n",
       "      <td>7 Eleven French Vanilla Iced Coffee</td>\n",
       "      <td>7 Eleven</td>\n",
       "      <td>1</td>\n",
       "      <td>French Vanilla Iced Coffee</td>\n",
       "      <td>French Vanilla Iced Coffee, Chillers Iced Coff...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35027</td>\n",
       "      <td>2017</td>\n",
       "      <td>7 Eleven French Vanilla Cappuccino</td>\n",
       "      <td>7 Eleven</td>\n",
       "      <td>1</td>\n",
       "      <td>French Vanilla Cappuccino</td>\n",
       "      <td>French Vanilla Cappuccino, Coffee, Drinks, Fla...</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35028</td>\n",
       "      <td>2017</td>\n",
       "      <td>7 Eleven Peppermint Mocha</td>\n",
       "      <td>7 Eleven</td>\n",
       "      <td>1</td>\n",
       "      <td>Peppermint Mocha</td>\n",
       "      <td>Peppermint Mocha, Coffee, 8 fl oz</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35029</td>\n",
       "      <td>2017</td>\n",
       "      <td>7 Eleven Pumpkin Spice Latte</td>\n",
       "      <td>7 Eleven</td>\n",
       "      <td>1</td>\n",
       "      <td>Pumpkin Spice Latte</td>\n",
       "      <td>Pumpkin Spice Latte, Coffee, 8 fl oz</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Menu_Item_ID  Year                 Restaurant_Item_Name restaurant  \\\n",
       "0         35005  2017           7 Eleven Mocha Iced Coffee   7 Eleven   \n",
       "1         35008  2017  7 Eleven French Vanilla Iced Coffee   7 Eleven   \n",
       "2         35027  2017   7 Eleven French Vanilla Cappuccino   7 Eleven   \n",
       "3         35028  2017            7 Eleven Peppermint Mocha   7 Eleven   \n",
       "4         35029  2017         7 Eleven Pumpkin Spice Latte   7 Eleven   \n",
       "\n",
       "   Restaurant_ID                   Item_Name  \\\n",
       "0              1           Mocha Iced Coffee   \n",
       "1              1  French Vanilla Iced Coffee   \n",
       "2              1   French Vanilla Cappuccino   \n",
       "3              1            Peppermint Mocha   \n",
       "4              1         Pumpkin Spice Latte   \n",
       "\n",
       "                                    Item_Description Food_Category  \\\n",
       "0    Mocha Iced Coffee, Chillers Iced Coffee, Drinks     Beverages   \n",
       "1  French Vanilla Iced Coffee, Chillers Iced Coff...     Beverages   \n",
       "2  French Vanilla Cappuccino, Coffee, Drinks, Fla...     Beverages   \n",
       "3                  Peppermint Mocha, Coffee, 8 fl oz     Beverages   \n",
       "4               Pumpkin Spice Latte, Coffee, 8 fl oz     Beverages   \n",
       "\n",
       "   Serving_Size Serving_Size_text  ... Sodium_text Potassium_text  \\\n",
       "0           NaN               NaN  ...         NaN            NaN   \n",
       "1           NaN               NaN  ...         NaN            NaN   \n",
       "2           8.0               NaN  ...         NaN            NaN   \n",
       "3           8.0               NaN  ...         NaN            NaN   \n",
       "4           8.0               NaN  ...         NaN            NaN   \n",
       "\n",
       "   Carbohydrates_text  Protein_text  Sugar_text  Dietary_Fiber_text  \\\n",
       "0                 NaN           NaN         NaN                 NaN   \n",
       "1                 NaN           NaN         NaN                 NaN   \n",
       "2                 NaN           NaN         NaN                 NaN   \n",
       "3                 NaN           NaN         NaN                 NaN   \n",
       "4                 NaN           NaN         NaN                 NaN   \n",
       "\n",
       "   Kids_Meal  Limited_Time_Offer  Regional  Shareable  \n",
       "0          0                   0         0          0  \n",
       "1          0                   0         0          0  \n",
       "2          0                   0         1          0  \n",
       "3          0                   0         0          0  \n",
       "4          0                   0         1          0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('Data/menu_items.csv', low_memory = False)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e4357f-681f-4706-b7ca-869442e11fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65219 entries, 0 to 65218\n",
      "Data columns (total 49 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Menu_Item_ID            65219 non-null  int64  \n",
      " 1   Year                    65219 non-null  int64  \n",
      " 2   Restaurant_Item_Name    65219 non-null  object \n",
      " 3   restaurant              65219 non-null  object \n",
      " 4   Restaurant_ID           65219 non-null  int64  \n",
      " 5   Item_Name               65219 non-null  object \n",
      " 6   Item_Description        65219 non-null  object \n",
      " 7   Food_Category           65219 non-null  object \n",
      " 8   Serving_Size            26899 non-null  float64\n",
      " 9   Serving_Size_text       39 non-null     object \n",
      " 10  Serving_Size_Unit       26927 non-null  object \n",
      " 11  Serving_Size_household  15238 non-null  object \n",
      " 12  Calories                55315 non-null  float64\n",
      " 13  Total_Fat               54846 non-null  float64\n",
      " 14  Saturated_Fat           54143 non-null  float64\n",
      " 15  Trans_Fat               51503 non-null  float64\n",
      " 16  Cholesterol             53219 non-null  float64\n",
      " 17  Sodium                  54991 non-null  float64\n",
      " 18  Potassium               1098 non-null   float64\n",
      " 19  Carbohydrates           54288 non-null  float64\n",
      " 20  Protein                 54233 non-null  float64\n",
      " 21  Sugar                   52931 non-null  float64\n",
      " 22  Dietary_Fiber           53440 non-null  float64\n",
      " 23  Calories_100g           25878 non-null  float64\n",
      " 24  Total_Fat_100g          25686 non-null  float64\n",
      " 25  Saturated_Fat_100g      25285 non-null  float64\n",
      " 26  Trans_Fat_100g          23828 non-null  float64\n",
      " 27  Cholesterol_100g        25126 non-null  float64\n",
      " 28  Sodium_100g             25853 non-null  float64\n",
      " 29  Potassium_100g          623 non-null    float64\n",
      " 30  Carbohydrates_100g      25592 non-null  float64\n",
      " 31  Protein_100g            25531 non-null  float64\n",
      " 32  Sugar_100g              25207 non-null  float64\n",
      " 33  Dietary_Fiber_100g      25391 non-null  float64\n",
      " 34  Calories_text           303 non-null    object \n",
      " 35  Total_Fat_text          69 non-null     object \n",
      " 36  Saturated_Fat_text      50 non-null     object \n",
      " 37  Trans_Fat_text          10 non-null     object \n",
      " 38  Cholesterol_text        358 non-null    object \n",
      " 39  Sodium_text             93 non-null     object \n",
      " 40  Potassium_text          2 non-null      object \n",
      " 41  Carbohydrates_text      634 non-null    object \n",
      " 42  Protein_text            467 non-null    object \n",
      " 43  Sugar_text              591 non-null    object \n",
      " 44  Dietary_Fiber_text      811 non-null    object \n",
      " 45  Kids_Meal               65219 non-null  int64  \n",
      " 46  Limited_Time_Offer      65219 non-null  int64  \n",
      " 47  Regional                65219 non-null  int64  \n",
      " 48  Shareable               65219 non-null  int64  \n",
      "dtypes: float64(23), int64(7), object(19)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e7e2329-c19f-4b91-9846-d298df9af903",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = ['Restaurant_Item_Name', 'restaurant', 'Item_Name', 'Item_Description', 'Food_Category', 'Sugar']\n",
    "df_relevant = df_raw.loc[:, relevant_columns]\n",
    "df_relevant.dropna(inplace = True)\n",
    "df = df_relevant.sort_values(by = 'Sugar', ascending = False)\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74671707-6363-4ea1-b47f-9f736b73fe23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant_Item_Name</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>Item_Name</th>\n",
       "      <th>Item_Description</th>\n",
       "      <th>Food_Category</th>\n",
       "      <th>Sugar</th>\n",
       "      <th>sugar_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dairy Queen Cookie Dough Blizzard Cake, 10 in</td>\n",
       "      <td>Dairy Queen</td>\n",
       "      <td>Cookie Dough Blizzard Cake, 10 in</td>\n",
       "      <td>Cookie Dough Blizzard Cake, 10 in w/ Vanilla S...</td>\n",
       "      <td>Desserts</td>\n",
       "      <td>783.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dairy Queen Reeses Peanut Butter Cups Blizzard...</td>\n",
       "      <td>Dairy Queen</td>\n",
       "      <td>Reeses Peanut Butter Cups Blizzard Cake, 10 in</td>\n",
       "      <td>Reeses Peanut Butter Cups Blizzard Cake, 10 in...</td>\n",
       "      <td>Desserts</td>\n",
       "      <td>737.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dairy Queen Chocolate Xtreme Blizzard Cake, 10 in</td>\n",
       "      <td>Dairy Queen</td>\n",
       "      <td>Chocolate Xtreme Blizzard Cake, 10 in</td>\n",
       "      <td>Chocolate Xtreme Blizzard Cake, 10 in w/ Brown...</td>\n",
       "      <td>Desserts</td>\n",
       "      <td>735.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dairy Queen Oreo Blizzard Cake, 10 in</td>\n",
       "      <td>Dairy Queen</td>\n",
       "      <td>Oreo Blizzard Cake, 10 in</td>\n",
       "      <td>Oreo Blizzard Cake, 10 in w/ Oreo Cookie Piece...</td>\n",
       "      <td>Desserts</td>\n",
       "      <td>720.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dairy Queen DQ Round Cake, 10 in</td>\n",
       "      <td>Dairy Queen</td>\n",
       "      <td>DQ Round Cake, 10 in</td>\n",
       "      <td>DQ Round Cake w/ Cake Crunch Filling, Chocolat...</td>\n",
       "      <td>Desserts</td>\n",
       "      <td>569.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52926</th>\n",
       "      <td>Popeyes 6 Nuggets</td>\n",
       "      <td>Popeyes</td>\n",
       "      <td>6 Nuggets</td>\n",
       "      <td>6 Nuggets, Tenders</td>\n",
       "      <td>Entrees</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52927</th>\n",
       "      <td>Popeyes Breast, Bonafide Spicy Chicken</td>\n",
       "      <td>Popeyes</td>\n",
       "      <td>Breast, Bonafide Spicy Chicken</td>\n",
       "      <td>Breast, Bonafide Spicy Chicken</td>\n",
       "      <td>Entrees</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52928</th>\n",
       "      <td>Popeyes Thigh, Bonafide Spicy Chicken</td>\n",
       "      <td>Popeyes</td>\n",
       "      <td>Thigh, Bonafide Spicy Chicken</td>\n",
       "      <td>Thigh, Bonafide Spicy Chicken, 300 Calories or...</td>\n",
       "      <td>Entrees</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52929</th>\n",
       "      <td>Popeyes Leg, Bonafide Spicy Chicken</td>\n",
       "      <td>Popeyes</td>\n",
       "      <td>Leg, Bonafide Spicy Chicken</td>\n",
       "      <td>Leg, Bonafide Spicy Chicken, 200 Calories or U...</td>\n",
       "      <td>Entrees</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52930</th>\n",
       "      <td>Sheetz Black Pepper, for MTO Shnack Wrapz</td>\n",
       "      <td>Sheetz</td>\n",
       "      <td>Black Pepper, for MTO Shnack Wrapz</td>\n",
       "      <td>Black Pepper, for MTO Shnack Wrapz &amp; MTO Slide...</td>\n",
       "      <td>Toppings &amp; Ingredients</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52931 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Restaurant_Item_Name   restaurant  \\\n",
       "0          Dairy Queen Cookie Dough Blizzard Cake, 10 in  Dairy Queen   \n",
       "1      Dairy Queen Reeses Peanut Butter Cups Blizzard...  Dairy Queen   \n",
       "2      Dairy Queen Chocolate Xtreme Blizzard Cake, 10 in  Dairy Queen   \n",
       "3                  Dairy Queen Oreo Blizzard Cake, 10 in  Dairy Queen   \n",
       "4                       Dairy Queen DQ Round Cake, 10 in  Dairy Queen   \n",
       "...                                                  ...          ...   \n",
       "52926                                  Popeyes 6 Nuggets      Popeyes   \n",
       "52927             Popeyes Breast, Bonafide Spicy Chicken      Popeyes   \n",
       "52928              Popeyes Thigh, Bonafide Spicy Chicken      Popeyes   \n",
       "52929                Popeyes Leg, Bonafide Spicy Chicken      Popeyes   \n",
       "52930          Sheetz Black Pepper, for MTO Shnack Wrapz       Sheetz   \n",
       "\n",
       "                                            Item_Name  \\\n",
       "0                   Cookie Dough Blizzard Cake, 10 in   \n",
       "1      Reeses Peanut Butter Cups Blizzard Cake, 10 in   \n",
       "2               Chocolate Xtreme Blizzard Cake, 10 in   \n",
       "3                           Oreo Blizzard Cake, 10 in   \n",
       "4                                DQ Round Cake, 10 in   \n",
       "...                                               ...   \n",
       "52926                                       6 Nuggets   \n",
       "52927                  Breast, Bonafide Spicy Chicken   \n",
       "52928                   Thigh, Bonafide Spicy Chicken   \n",
       "52929                     Leg, Bonafide Spicy Chicken   \n",
       "52930              Black Pepper, for MTO Shnack Wrapz   \n",
       "\n",
       "                                        Item_Description  \\\n",
       "0      Cookie Dough Blizzard Cake, 10 in w/ Vanilla S...   \n",
       "1      Reeses Peanut Butter Cups Blizzard Cake, 10 in...   \n",
       "2      Chocolate Xtreme Blizzard Cake, 10 in w/ Brown...   \n",
       "3      Oreo Blizzard Cake, 10 in w/ Oreo Cookie Piece...   \n",
       "4      DQ Round Cake w/ Cake Crunch Filling, Chocolat...   \n",
       "...                                                  ...   \n",
       "52926                                 6 Nuggets, Tenders   \n",
       "52927                     Breast, Bonafide Spicy Chicken   \n",
       "52928  Thigh, Bonafide Spicy Chicken, 300 Calories or...   \n",
       "52929  Leg, Bonafide Spicy Chicken, 200 Calories or U...   \n",
       "52930  Black Pepper, for MTO Shnack Wrapz & MTO Slide...   \n",
       "\n",
       "                Food_Category  Sugar  sugar_class  \n",
       "0                    Desserts  783.0            5  \n",
       "1                    Desserts  737.0            5  \n",
       "2                    Desserts  735.0            5  \n",
       "3                    Desserts  720.0            5  \n",
       "4                    Desserts  569.0            5  \n",
       "...                       ...    ...          ...  \n",
       "52926                 Entrees    0.0            1  \n",
       "52927                 Entrees    0.0            1  \n",
       "52928                 Entrees    0.0            1  \n",
       "52929                 Entrees    0.0            1  \n",
       "52930  Toppings & Ingredients    0.0            1  \n",
       "\n",
       "[52931 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sugar_classifier(sugar):\n",
    "    if sugar >= 30:\n",
    "        return 5\n",
    "    elif (sugar < 30) & (sugar >= 7):\n",
    "        return 4\n",
    "    elif (sugar < 7) & (sugar > 2):\n",
    "        return 3\n",
    "    elif (sugar <= 2) & (sugar > 0):\n",
    "        return 2\n",
    "    elif sugar == 0:\n",
    "        return 1\n",
    "\n",
    "df['sugar_class'] = df['Sugar'].apply(sugar_classifier)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b34538ee-22e5-4b0a-8688-1b554dfc78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df_relevant['restaurant'].value_counts()\n",
    "for value, count in value_counts.items():\n",
    "    print(f'{value}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5ed4fee-f205-4125-8fa5-bd4cf7150162",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['Restaurant_Item_Name'] + \" \" + df['Item_Name'] + \" \" + df['Item_Description'] + \" \" + df['Food_Category']\n",
    "df.drop(columns = ['Restaurant_Item_Name', 'Item_Name', 'Item_Description', 'Food_Category', 'Sugar'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e7edac3-76cf-4b05-b70e-60466c4100dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['restaurant'] = df['restaurant'].str.split()\n",
    "for index, row in df.iterrows():\n",
    "    for string in row['restaurant']:\n",
    "        df.at[index, 'text'] = df.at[index, 'text'].replace(string, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4b8f289-0f12-4b97-9037-b561f4c98ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sugar_class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Cookie Dough Blizzard Cake, 10 in Cookie Dou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Reeses Peanut Butter Cups Blizzard Cake, 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Chocolate Xtreme Blizzard Cake, 10 in Chocol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Oreo Blizzard Cake, 10 in Oreo Blizzard Cake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>DQ Round Cake, 10 in DQ Round Cake, 10 in DQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52926</th>\n",
       "      <td>1</td>\n",
       "      <td>6 Nuggets 6 Nuggets 6 Nuggets, Tenders Entrees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52927</th>\n",
       "      <td>1</td>\n",
       "      <td>Breast, Bonafide Spicy Chicken Breast, Bonafi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52928</th>\n",
       "      <td>1</td>\n",
       "      <td>Thigh, Bonafide Spicy Chicken Thigh, Bonafide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52929</th>\n",
       "      <td>1</td>\n",
       "      <td>Leg, Bonafide Spicy Chicken Leg, Bonafide Spi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52930</th>\n",
       "      <td>1</td>\n",
       "      <td>Black Pepper, for MTO Shnack Wrapz Black Pepp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52931 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sugar_class                                               text\n",
       "0                5    Cookie Dough Blizzard Cake, 10 in Cookie Dou...\n",
       "1                5    Reeses Peanut Butter Cups Blizzard Cake, 10 ...\n",
       "2                5    Chocolate Xtreme Blizzard Cake, 10 in Chocol...\n",
       "3                5    Oreo Blizzard Cake, 10 in Oreo Blizzard Cake...\n",
       "4                5    DQ Round Cake, 10 in DQ Round Cake, 10 in DQ...\n",
       "...            ...                                                ...\n",
       "52926            1     6 Nuggets 6 Nuggets 6 Nuggets, Tenders Entrees\n",
       "52927            1   Breast, Bonafide Spicy Chicken Breast, Bonafi...\n",
       "52928            1   Thigh, Bonafide Spicy Chicken Thigh, Bonafide...\n",
       "52929            1   Leg, Bonafide Spicy Chicken Leg, Bonafide Spi...\n",
       "52930            1   Black Pepper, for MTO Shnack Wrapz Black Pepp...\n",
       "\n",
       "[52931 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns = 'restaurant', inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77c15ccb-09c9-4720-97e4-631dbfbf8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('prepared_text_data_sugar.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d132c490-f754-438b-8fbb-b70bff8be60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, data, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data, y=None):\n",
    "        preprocessed_data = [self.stem_doc(doc) for doc in data]\n",
    "        return preprocessed_data\n",
    "\n",
    "    def stem_doc(self, doc):\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        lower_doc = [token.lower() for token in word_tokenize(doc) if token.isalpha()]\n",
    "        filtered_doc = [token for token in lower_doc if token not in stop_words]\n",
    "        stemmed_doc = [stemmer.stem(token) for token in filtered_doc]\n",
    "        return \" \".join(stemmed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2cb43fb-177c-4b5f-bb72-75475f30f1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.5982513713262635\n",
      "Log Loss:  0.9766313775364627\n"
     ]
    }
   ],
   "source": [
    "X = df['text']\n",
    "y = df['sugar_class']\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 200)\n",
    "processor = Preprocessor()\n",
    "X_train_transformed = processor.fit_transform(X_train_raw)\n",
    "X_test_transformed = processor.transform(X_test_raw)\n",
    "\n",
    "vector_pipe = Pipeline([('tfidf', TfidfVectorizer())])\n",
    "X_train_vector = vector_pipe.fit_transform(X_train_transformed)\n",
    "X_test_vector = vector_pipe.transform(X_test_transformed)\n",
    "X_train = pd.DataFrame(X_train_vector.toarray(), columns = vector_pipe['tfidf'].get_feature_names())\n",
    "X_test = pd.DataFrame(X_test_vector.toarray(), columns = vector_pipe['tfidf'].get_feature_names())\n",
    "\n",
    "multinb = MultinomialNB()\n",
    "multinb.fit(X_train, y_train)\n",
    "y_pred_baseline = multinb.predict(X_test)\n",
    "y_pred_proba_baseline = multinb.predict_proba(X_test)\n",
    "\n",
    "print(f\"Weighted F1 Score: {f1_score(y_test, y_pred_baseline, average = 'weighted')}\")\n",
    "print(f\"Log Loss:  {log_loss(y_test, y_pred_proba_baseline)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a674d374-88b2-419a-81ed-3723e1670d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.6021715305623653\n",
      "Log Loss:  1.1526683451434967\n"
     ]
    }
   ],
   "source": [
    "compnb = ComplementNB()\n",
    "compnb.fit(X_train, y_train)\n",
    "y_pred_compnb = compnb.predict(X_test)\n",
    "y_pred_proba_compnb = compnb.predict_proba(X_test)\n",
    "\n",
    "print(f\"Weighted F1 Score: {f1_score(y_test, y_pred_compnb, average = 'weighted')}\")\n",
    "print(f\"Log Loss:  {log_loss(y_test, y_pred_proba_compnb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2210dfc9-1462-43c7-b266-9ff5e93265d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_to_skip = sorted(random.sample(range(1, 52932), 32931))\n",
    "subset = pd.read_csv('Data/prepared_text_data_sugar.csv', skiprows = lines_to_skip)\n",
    "\n",
    "X_sub = subset['text']\n",
    "y_sub = subset['Sugar_Class']\n",
    "X_train_sub_raw, X_test_sub_raw, y_train_sub, y_test_sub = train_test_split(X_sub, y_sub, test_size = 0.2, random_state = 100)\n",
    "X_train_transformed_sub = processor.fit_transform(X_train_sub_raw)\n",
    "X_test_transformed_sub = processor.transform(X_test_sub_raw)\n",
    "\n",
    "vector_pipe_sub = Pipeline([('tfidf', TfidfVectorizer())])\n",
    "X_train_vector_sub = vector_pipe_sub.fit_transform(X_train_transformed_sub)\n",
    "X_test_vector_sub = vector_pipe_sub.transform(X_test_transformed_sub)\n",
    "X_train_sub = pd.DataFrame(X_train_vector_sub.toarray(), columns = vector_pipe_sub['tfidf'].get_feature_names())\n",
    "X_test_sub = pd.DataFrame(X_test_vector_sub.toarray(), columns = vector_pipe_sub['tfidf'].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96e038b0-d8e8-4c42-bb92-029a957c2b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Search found the following optimal parameters: \n",
      "learning_rate: 0.2977513564547608\n",
      "max_depth: 8\n",
      "min_child_weight: 2\n",
      "\n",
      "Weighted F1 Score (Train): 0.8400278977452622\n",
      "Weighted F1 Score (Test): 0.6952111329477347\n",
      "Log Loss (Train):  0.5694687150081663\n",
      "Log Loss (Test):  0.782527227344457\n",
      "\n",
      "Weighted F1 Score After Hyperparameter Tuning (Train): 0.8729975145964162\n",
      "Weighted F1 Score After Hyperparameter Tuning (Test): 0.7067383656667726\n",
      "Log Loss After Hyperparameter Tuning (Train):  0.49105750232531137\n",
      "Log Loss After Hyperparameter Tuning (Test):  0.7490659579653876\n",
      "Run Time: 6923.436443805695\n"
     ]
    }
   ],
   "source": [
    "start_time_xgb = time.time()\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train_sub, y_train_sub)\n",
    "y_pred_xgb_train = xgb.predict(X_train_sub)\n",
    "y_pred_xgb_test = xgb.predict(X_test_sub)\n",
    "y_proba_xgb_train = xgb.predict_proba(X_train_sub)\n",
    "y_proba_xgb_test = xgb.predict_proba(X_test_sub)\n",
    "\n",
    "xgb_params = {\n",
    "    'learning_rate': uniform(0.1, 0.2),\n",
    "    'max_depth': randint(3, 9),\n",
    "    'min_child_weight': randint(1, 3),\n",
    "}\n",
    "\n",
    "search_xgb = RandomizedSearchCV(xgb, xgb_params, scoring = 'f1_weighted', n_jobs = 1)\n",
    "search_xgb.fit(X_train_sub, y_train_sub)\n",
    "best_parameters = search_xgb.best_params_\n",
    "\n",
    "print('Randomized Search found the following optimal parameters: ')\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('%s: %r' % (param_name, best_parameters[param_name]))\n",
    "print(\"\")\n",
    "\n",
    "y_pred_xgb_param_train = search_xgb.predict(X_train_sub)\n",
    "y_pred_xgb_param_test = search_xgb.predict(X_test_sub)\n",
    "y_proba_xgb_param_train = search_xgb.predict_proba(X_train_sub)\n",
    "y_proba_xgb_param_test = search_xgb.predict_proba(X_test_sub)\n",
    "\n",
    "end_time_xgb = time.time()\n",
    "\n",
    "print(f\"Weighted F1 Score (Train): {f1_score(y_train_sub, y_pred_xgb_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score (Test): {f1_score(y_test_sub, y_pred_xgb_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss (Train):  {log_loss(y_train_sub, y_proba_xgb_train)}\")\n",
    "print(f\"Log Loss (Test):  {log_loss(y_test_sub, y_proba_xgb_test)}\")\n",
    "print(\"\")\n",
    "print(f\"Weighted F1 Score After Hyperparameter Tuning (Train): {f1_score(y_train_sub, y_pred_xgb_param_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score After Hyperparameter Tuning (Test): {f1_score(y_test_sub, y_pred_xgb_param_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss After Hyperparameter Tuning (Train):  {log_loss(y_train_sub, y_proba_xgb_param_train)}\")\n",
    "print(f\"Log Loss After Hyperparameter Tuning (Test):  {log_loss(y_test_sub, y_proba_xgb_param_test)}\")\n",
    "print(f\"Run Time: {end_time_xgb - start_time_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c20fa848-2e4b-4181-8b08-4ddef28afb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.5283394948236613\n",
      "Log Loss:  1.5541493294978928\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "y_pred_proba_ada = ada.predict_proba(X_test)\n",
    "\n",
    "print(f\"Weighted F1 Score: {f1_score(y_test, y_pred_ada, average = 'weighted')}\")\n",
    "print(f\"Log Loss:  {log_loss(y_test, y_pred_proba_ada)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1e85149-c8be-4de2-aba3-729a0e06c779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Search found the following optimal parameters: \n",
      "max_depth: 8\n",
      "min_samples_leaf: 1\n",
      "min_samples_split: 2\n",
      "n_estimators: 100\n",
      "\n",
      "Weighted F1 Score (Train): 0.9931866485409155\n",
      "Weighted F1 Score (Test): 0.726367897119568\n",
      "Log Loss (Train):  0.18638262212047357\n",
      "Log Loss (Test):  0.7592780225634445\n",
      "\n",
      "Weighted F1 Score After Hyperparameter Tuning (Train): 0.46278718366778837\n",
      "Weighted F1 Score After Hyperparameter Tuning (Test): 0.454376778264133\n",
      "Log Loss After Hyperparameter Tuning (Train):  1.291941409039672\n",
      "Log Loss After Hyperparameter Tuning (Test):  1.3010797081137913\n",
      "Run Time: 237.08182501792908\n"
     ]
    }
   ],
   "source": [
    "start_time_rfc = time.time()\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_sub, y_train_sub)\n",
    "y_pred_rfc_train = rfc.predict(X_train_sub)\n",
    "y_pred_rfc_test = rfc.predict(X_test_sub)\n",
    "y_proba_rfc_train = rfc.predict_proba(X_train_sub)\n",
    "y_proba_rfc_test = rfc.predict_proba(X_test_sub)\n",
    "\n",
    "rfc_params = {\n",
    "    'n_estimators':  [10, 50, 100],\n",
    "    'max_depth':  randint(3, 9),\n",
    "    'min_samples_split':  randint(2, 4),\n",
    "    'min_samples_leaf':  randint(1, 3),\n",
    "}\n",
    "\n",
    "search_rfc = RandomizedSearchCV(rfc, rfc_params, scoring = 'f1_weighted', n_jobs = 1)\n",
    "search_rfc.fit(X_train_sub, y_train_sub)\n",
    "best_parameters = search_rfc.best_params_\n",
    "\n",
    "print('Randomized Search found the following optimal parameters: ')\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('%s: %r' % (param_name, best_parameters[param_name]))\n",
    "print(\"\")\n",
    "\n",
    "y_pred_rfc_param_train = search_rfc.predict(X_train_sub)\n",
    "y_pred_rfc_param_test = search_rfc.predict(X_test_sub)\n",
    "y_proba_rfc_param_train = search_rfc.predict_proba(X_train_sub)\n",
    "y_proba_rfc_param_test = search_rfc.predict_proba(X_test_sub)\n",
    "\n",
    "end_time_rfc = time.time()\n",
    "\n",
    "print(f\"Weighted F1 Score (Train): {f1_score(y_train_sub, y_pred_rfc_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score (Test): {f1_score(y_test_sub, y_pred_rfc_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss (Train):  {log_loss(y_train_sub, y_proba_rfc_train)}\")\n",
    "print(f\"Log Loss (Test):  {log_loss(y_test_sub, y_proba_rfc_test)}\")\n",
    "print(\"\")\n",
    "print(f\"Weighted F1 Score After Hyperparameter Tuning (Train): {f1_score(y_train_sub, y_pred_rfc_param_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score After Hyperparameter Tuning (Test): {f1_score(y_test_sub, y_pred_rfc_param_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss After Hyperparameter Tuning (Train):  {log_loss(y_train_sub, y_proba_rfc_param_train)}\")\n",
    "print(f\"Log Loss After Hyperparameter Tuning (Test):  {log_loss(y_test_sub, y_proba_rfc_param_test)}\")\n",
    "print(f\"Run Time: {end_time_rfc - start_time_rfc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f012d084-0c22-4738-9529-3c70af56d723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time: 205.5940968990326\n",
      "Weighted F1 Score (Train): 0.9459045001884633\n",
      "Weighted F1 Score (Test): 0.7475568522988201\n",
      "Log Loss (Train):  0.414485587412872\n",
      "Log Loss (Test):  0.6888136677530962\n"
     ]
    }
   ],
   "source": [
    "start_time_rfc1 = time.time()\n",
    "\n",
    "rfc1 = RandomForestClassifier(n_estimators = 200,\n",
    "                              criterion = 'gini',\n",
    "                              max_depth = None,\n",
    "                              min_samples_split = 4,\n",
    "                              min_samples_leaf = 1,\n",
    "                              max_leaf_nodes = None)\n",
    "rfc1.fit(X_train, y_train)\n",
    "y_pred_rfc1_train = rfc1.predict(X_train)\n",
    "y_pred_rfc1_test = rfc1.predict(X_test)\n",
    "y_proba_rfc1_train = rfc1.predict_proba(X_train)\n",
    "y_proba_rfc1_test = rfc1.predict_proba(X_test)\n",
    "\n",
    "end_time_rfc1 = time.time()\n",
    "\n",
    "print(f\"Run Time: {end_time_rfc1 - start_time_rfc1}\")\n",
    "print(f\"Weighted F1 Score (Train): {f1_score(y_train, y_pred_rfc1_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score (Test): {f1_score(y_test, y_pred_rfc1_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss (Train):  {log_loss(y_train, y_proba_rfc1_train)}\")\n",
    "print(f\"Log Loss (Test):  {log_loss(y_test, y_proba_rfc1_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e955663a-f9d9-4d3b-97b6-63109f1053c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score (Train): 0.8363684539787087\n",
      "Weighted F1 Score (Test): 0.7274706111825796\n",
      "Log Loss (Train):  0.5402181994621008\n",
      "Log Loss (Test):  0.7095042726206793\n",
      "Run Time: 347.4807481765747\n"
     ]
    }
   ],
   "source": [
    "start_time_xgb = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 10, learning_rate = 0.29775, max_depth = 8, min_child_weight = 2, random_state = 100)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_train = xgb.predict(X_train)\n",
    "y_pred_xgb_test = xgb.predict(X_test)\n",
    "y_proba_xgb_train = xgb.predict_proba(X_train)\n",
    "y_proba_xgb_test = xgb.predict_proba(X_test)\n",
    "\n",
    "end_time_xgb = time.time()\n",
    "\n",
    "print(f\"Weighted F1 Score (Train): {f1_score(y_train, y_pred_xgb_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score (Test): {f1_score(y_test, y_pred_xgb_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss (Train):  {log_loss(y_train, y_proba_xgb_train)}\")\n",
    "print(f\"Log Loss (Test):  {log_loss(y_test, y_proba_xgb_test)}\")\n",
    "print(f\"Run Time: {end_time_xgb - start_time_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a972324-56b8-43da-b66c-3c1a84a470b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score (Train): 0.7861715069951298\n",
      "Weighted F1 Score (Test): 0.6996941067106328\n",
      "Log Loss (Train):  0.6465076149790803\n",
      "Log Loss (Test):  0.7721565087923342\n",
      "Run Time: 191.12715482711792\n"
     ]
    }
   ],
   "source": [
    "start_time_xgb = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 50, learning_rate = 0.29775, max_depth = 8, min_child_weight = 2, random_state = 100)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_train = xgb.predict(X_train)\n",
    "y_pred_xgb_test = xgb.predict(X_test)\n",
    "y_proba_xgb_train = xgb.predict_proba(X_train)\n",
    "y_proba_xgb_test = xgb.predict_proba(X_test)\n",
    "\n",
    "end_time_xgb = time.time()\n",
    "\n",
    "print(f\"Weighted F1 Score (Train): {f1_score(y_train, y_pred_xgb_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score (Test): {f1_score(y_test, y_pred_xgb_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss (Train):  {log_loss(y_train, y_proba_xgb_train)}\")\n",
    "print(f\"Log Loss (Test):  {log_loss(y_test, y_proba_xgb_test)}\")\n",
    "print(f\"Run Time: {end_time_xgb - start_time_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7836ba7e-9939-435b-bbe4-b3c48069b22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score (Train): 0.8363684539787087\n",
      "Weighted F1 Score (Test): 0.7274706111825796\n",
      "Log Loss (Train):  0.5402181994621008\n",
      "Log Loss (Test):  0.7095042726206793\n",
      "Run Time: 388.44012236595154\n"
     ]
    }
   ],
   "source": [
    "start_time_xgb = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 100, learning_rate = 0.29775, max_depth = 8, min_child_weight = 2, random_state = 100)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_train = xgb.predict(X_train)\n",
    "y_pred_xgb_test = xgb.predict(X_test)\n",
    "y_proba_xgb_train = xgb.predict_proba(X_train)\n",
    "y_proba_xgb_test = xgb.predict_proba(X_test)\n",
    "\n",
    "end_time_xgb = time.time()\n",
    "\n",
    "print(f\"Weighted F1 Score (Train): {f1_score(y_train, y_pred_xgb_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score (Test): {f1_score(y_test, y_pred_xgb_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss (Train):  {log_loss(y_train, y_proba_xgb_train)}\")\n",
    "print(f\"Log Loss (Test):  {log_loss(y_test, y_proba_xgb_test)}\")\n",
    "print(f\"Run Time: {end_time_xgb - start_time_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0309b505-046e-4078-ab6a-26734ff06b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score (Train): 0.8666329393072114\n",
      "Weighted F1 Score (Test): 0.740821573576041\n",
      "Log Loss (Train):  0.4751014555525105\n",
      "Log Loss (Test):  0.6757501598639375\n",
      "Run Time: 528.4391038417816\n"
     ]
    }
   ],
   "source": [
    "start_time_xgb = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 150, learning_rate = 0.29775, max_depth = 8, min_child_weight = 2, random_state = 100)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_train = xgb.predict(X_train)\n",
    "y_pred_xgb_test = xgb.predict(X_test)\n",
    "y_proba_xgb_train = xgb.predict_proba(X_train)\n",
    "y_proba_xgb_test = xgb.predict_proba(X_test)\n",
    "\n",
    "end_time_xgb = time.time()\n",
    "\n",
    "print(f\"Weighted F1 Score (Train): {f1_score(y_train, y_pred_xgb_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score (Test): {f1_score(y_test, y_pred_xgb_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss (Train):  {log_loss(y_train, y_proba_xgb_train)}\")\n",
    "print(f\"Log Loss (Test):  {log_loss(y_test, y_proba_xgb_test)}\")\n",
    "print(f\"Run Time: {end_time_xgb - start_time_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74b2f90a-d6cf-47f2-a147-fe2e3797f6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score (Train): 0.979248789252413\n",
      "Weighted F1 Score (Test): 0.7687169973325381\n",
      "Log Loss (Train):  0.31491471134624954\n",
      "Log Loss (Test):  0.6881945138963503\n",
      "Run Time: 97.14397621154785\n"
     ]
    }
   ],
   "source": [
    "start_time_etc = time.time()\n",
    "\n",
    "etc = ExtraTreesClassifier(max_features = 'sqrt',\n",
    "                         max_samples = 0.5,\n",
    "                         bootstrap = True,\n",
    "                         random_state = 100)\n",
    "etc.fit(X_train, y_train)\n",
    "y_pred_etc_train = etc.predict(X_train)\n",
    "y_pred_etc_test = etc.predict(X_test)\n",
    "y_proba_etc_train = etc.predict_proba(X_train)\n",
    "y_proba_etc_test = etc.predict_proba(X_test)\n",
    "\n",
    "end_time_etc = time.time()\n",
    "\n",
    "print(f\"Weighted F1 Score (Train): {f1_score(y_train, y_pred_etc_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score (Test): {f1_score(y_test, y_pred_etc_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss (Train):  {log_loss(y_train, y_proba_etc_train)}\")\n",
    "print(f\"Log Loss (Test):  {log_loss(y_test, y_proba_etc_test)}\")\n",
    "print(f\"Run Time: {end_time_etc - start_time_etc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7293c9be-cab3-4db1-acad-d6885639e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score (Train): 0.8878741622932251\n",
      "Weighted F1 Score (Test): 0.7473048244180484\n",
      "Log Loss (Train):  0.42613900739648186\n",
      "Log Loss (Test):  0.6531871223705179\n",
      "Run Time: 1301.7270367145538\n"
     ]
    }
   ],
   "source": [
    "start_time_xgb = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 200, learning_rate = 0.29775, max_depth = 8, min_child_weight = 2, random_state = 100)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_train = xgb.predict(X_train)\n",
    "y_pred_xgb_test = xgb.predict(X_test)\n",
    "y_proba_xgb_train = xgb.predict_proba(X_train)\n",
    "y_proba_xgb_test = xgb.predict_proba(X_test)\n",
    "\n",
    "end_time_xgb = time.time()\n",
    "\n",
    "print(f\"Weighted F1 Score (Train): {f1_score(y_train, y_pred_xgb_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score (Test): {f1_score(y_test, y_pred_xgb_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss (Train):  {log_loss(y_train, y_proba_xgb_train)}\")\n",
    "print(f\"Log Loss (Test):  {log_loss(y_test, y_proba_xgb_test)}\")\n",
    "print(f\"Run Time: {end_time_xgb - start_time_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37a5ff8a-c92a-4495-b2f0-8f2eda2d1633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score (Train): 0.905086228975606\n",
      "Weighted F1 Score (Test): 0.7540658157886949\n",
      "Log Loss (Train):  0.3878126523060198\n",
      "Log Loss (Test):  0.6383257557702969\n",
      "Run Time: 986.0501399040222\n"
     ]
    }
   ],
   "source": [
    "start_time_xgb = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 250, learning_rate = 0.29775, max_depth = 8, min_child_weight = 2, random_state = 100)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_train = xgb.predict(X_train)\n",
    "y_pred_xgb_test = xgb.predict(X_test)\n",
    "y_proba_xgb_train = xgb.predict_proba(X_train)\n",
    "y_proba_xgb_test = xgb.predict_proba(X_test)\n",
    "\n",
    "end_time_xgb = time.time()\n",
    "\n",
    "print(f\"Weighted F1 Score (Train): {f1_score(y_train, y_pred_xgb_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score (Test): {f1_score(y_test, y_pred_xgb_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss (Train):  {log_loss(y_train, y_proba_xgb_train)}\")\n",
    "print(f\"Log Loss (Test):  {log_loss(y_test, y_proba_xgb_test)}\")\n",
    "print(f\"Run Time: {end_time_xgb - start_time_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84ca5a17-fa58-496b-bb95-7f08836930f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score (Train): 0.9170080121344002\n",
      "Weighted F1 Score (Test): 0.7585179756516903\n",
      "Log Loss (Train):  0.3556067710529828\n",
      "Log Loss (Test):  0.6256514843201355\n",
      "Run Time: 1043.0394687652588\n"
     ]
    }
   ],
   "source": [
    "start_time_xgb = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 300, learning_rate = 0.29775, max_depth = 8, min_child_weight = 2, random_state = 100)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_train = xgb.predict(X_train)\n",
    "y_pred_xgb_test = xgb.predict(X_test)\n",
    "y_proba_xgb_train = xgb.predict_proba(X_train)\n",
    "y_proba_xgb_test = xgb.predict_proba(X_test)\n",
    "\n",
    "end_time_xgb = time.time()\n",
    "\n",
    "print(f\"Weighted F1 Score (Train): {f1_score(y_train, y_pred_xgb_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score (Test): {f1_score(y_test, y_pred_xgb_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss (Train):  {log_loss(y_train, y_proba_xgb_train)}\")\n",
    "print(f\"Log Loss (Test):  {log_loss(y_test, y_proba_xgb_test)}\")\n",
    "print(f\"Run Time: {end_time_xgb - start_time_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98055523-73d9-4a7a-b0aa-7d09eb380d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score (Train): 0.928884180773971\n",
      "Weighted F1 Score (Test): 0.7625226375181446\n",
      "Log Loss (Train):  0.32737022889591566\n",
      "Log Loss (Test):  0.6157804221548738\n",
      "Run Time: 1100.0064754486084\n"
     ]
    }
   ],
   "source": [
    "start_time_xgb = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 350, learning_rate = 0.29775, max_depth = 8, min_child_weight = 2, random_state = 100)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_train = xgb.predict(X_train)\n",
    "y_pred_xgb_test = xgb.predict(X_test)\n",
    "y_proba_xgb_train = xgb.predict_proba(X_train)\n",
    "y_proba_xgb_test = xgb.predict_proba(X_test)\n",
    "\n",
    "end_time_xgb = time.time()\n",
    "\n",
    "print(f\"Weighted F1 Score (Train): {f1_score(y_train, y_pred_xgb_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score (Test): {f1_score(y_test, y_pred_xgb_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss (Train):  {log_loss(y_train, y_proba_xgb_train)}\")\n",
    "print(f\"Log Loss (Test):  {log_loss(y_test, y_proba_xgb_test)}\")\n",
    "print(f\"Run Time: {end_time_xgb - start_time_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d2b35f9-74d3-4900-9e18-481513bfe168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score (Train): 0.9372284327543249\n",
      "Weighted F1 Score (Test): 0.7645371487927606\n",
      "Log Loss (Train):  0.3036385798768231\n",
      "Log Loss (Test):  0.6093739166162941\n",
      "Run Time: 1256.4366989135742\n"
     ]
    }
   ],
   "source": [
    "start_time_xgb = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 400, learning_rate = 0.29775, max_depth = 8, min_child_weight = 2, random_state = 100)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_train = xgb.predict(X_train)\n",
    "y_pred_xgb_test = xgb.predict(X_test)\n",
    "y_proba_xgb_train = xgb.predict_proba(X_train)\n",
    "y_proba_xgb_test = xgb.predict_proba(X_test)\n",
    "\n",
    "end_time_xgb = time.time()\n",
    "\n",
    "print(f\"Weighted F1 Score (Train): {f1_score(y_train, y_pred_xgb_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score (Test): {f1_score(y_test, y_pred_xgb_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss (Train):  {log_loss(y_train, y_proba_xgb_train)}\")\n",
    "print(f\"Log Loss (Test):  {log_loss(y_test, y_proba_xgb_test)}\")\n",
    "print(f\"Run Time: {end_time_xgb - start_time_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9e7a2368-3bb0-44ab-8f68-ace83c768b11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score (Train): 0.9516040046818818\n",
      "Weighted F1 Score (Test): 0.7666405882305194\n",
      "Log Loss (Train):  0.26181900007080255\n",
      "Log Loss (Test):  0.6002008883437594\n",
      "Run Time: 1566.1028113365173\n"
     ]
    }
   ],
   "source": [
    "start_time_xgb = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 500, learning_rate = 0.29775, max_depth = 8, min_child_weight = 2, random_state = 100)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_train = xgb.predict(X_train)\n",
    "y_pred_xgb_test = xgb.predict(X_test)\n",
    "y_proba_xgb_train = xgb.predict_proba(X_train)\n",
    "y_proba_xgb_test = xgb.predict_proba(X_test)\n",
    "\n",
    "end_time_xgb = time.time()\n",
    "\n",
    "print(f\"Weighted F1 Score (Train): {f1_score(y_train, y_pred_xgb_train, average = 'weighted')}\")\n",
    "print(f\"Weighted F1 Score (Test): {f1_score(y_test, y_pred_xgb_test, average = 'weighted')}\")\n",
    "print(f\"Log Loss (Train):  {log_loss(y_train, y_proba_xgb_train)}\")\n",
    "print(f\"Log Loss (Test):  {log_loss(y_test, y_proba_xgb_test)}\")\n",
    "print(f\"Run Time: {end_time_xgb - start_time_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd74d6-d472-4efa-ae0a-28e495306e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
